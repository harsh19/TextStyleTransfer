\section{Experiments} \label{sec:Experiments}
\subsection{Preprocessing}
We lowercase sentences and then use NLTK's PUNKT tokenizer to tokenize all sentences. The \textit{Original} side has certain characters like \ae which are not extant in today's language. We map these characters to the closest equivalent character(s) used today (e.g \ae $\rightarrow$ ae)

\subsection{Baseline Methods}
\subsubsection{As-it-is}
Since both source and target side are English, just replicating the input on the target side is a valid and competitive baseline, with a BLEU of $21+$.

\subsubsection{Dictionary}
\cite{xu2012paraphrasing} also use an external, unsupervised dictionary from an external source, which is shared with the dataset. We augment this dictionary with pairs corresponding to the $2$nd person thou (\textit{thou}, \textit{thy}, \textit{thyself}) since these are amongst the most common tokens in the training set.

Directly using this dictionary to perform word-by-word replacement is another admittable baseline. As was noted in the original paper,  this baseline actually performs worse than \textit{As-it-is}. This could be due to its performing aggressive replacement without regard for word context. Moreover, a dictionary cannot easily capture one-to-many mappings as well as long-range dependencies \footnote{thou-thyself and you-yourself}.

\subsubsection{Off-the-shelf SMT}
To train statistical machine translation (\textit{SMT}) baselines, we use the publically available open-source toolkit MOSES \cite{koehn2007moses}, along with the GIZA++  word aligner \cite{och2003minimum}, as was done in \cite{xu2012paraphrasing}. For training the target-side LM component, we use the \textit{lmplz} toolkit within MOSES to train a 4-gram LM. We also use \textit{MERT}  \cite{och2003minimum}, available as part of MOSES, to tune on the validation set.

For fairness of comparison, it is necessary to use the pairwise dictionary and \textit{PTB} while training the SMT models as well. Although SMT does not have mechanisms directly comparable to neural models for incorporating these, the most obvious ways are to use the  dictionary and \textit{PTB} as additional training data for the alignment component and the target-side LM respectively. We experiment with several SMT models, ablating for the use of both \textit{PTB} and dictionary. In \ref{sec:Results}, we only report the performance of the best of these approaches, which uses a 4-gram target-side LM (trained without \textit{PTB}) and an alignment model trained with additional dictionary pairs. The Appendix reports performance for all the approaches.

%We report results with trigram and 4-gram LMs. The LMs are trained using the SRILM toolkit \cite{stolcke2002srilm} available in MOSES. 

\subsection{Evaluation}
Our primary evaluation metric is \emph{BLEU} \cite{papineni2002bleu} . As is standard in the MT community, we compute \emph{BLEU} using the freely available perl script\footnote{\url{http://tinyurl.com/yben45gm}} from the MOSES decoder.

We also use \emph{PINC} \cite{chen2011collecting} as an auxiliary evaluation metric. This metric originates from paraphrase evaluation literature and evaluates how much the target side paraphrases resemble the source side. Given a source sentence $s$ and a target side paraphrase $c$ generated by the system, \emph{PINC(s,c)} is defined as
\begin{center}
\scriptsize
\begin{align*}
    PINC(s,c)&=1-\frac{1}{N} \sum_{n=1}^{n=N} \frac{|Ngram(c,n) \cap Ngram(s,n)|}{|Ngram(c,n)|}
\end{align*}
\normalsize
\end{center}
where $Ngram(x,n)$ denotes the set of n-grams of length $n$ in sentence $x$, and $N$ is the maximum length of ngram considered. We set $N=4$. Higher the \textit{PINC}, greater the novelty of paraphrases generated by the system. Note, however, that PINC does not measure fluency of generated paraphrases.
%\subsection{Pretraining}
%We pretrain our embeddings on all training sentences. We also experiment with adding additional data from PTB \cite{marcus1993building} for better learning of embeddings. We use the preprocessed PTB data from \cite{mikolov2010recurrent}, which is a standard dataset used in language modelling.

%\subsubsection{PLAIN}
%\subsubsection{PLAINEXT}
%\subsubsection{RETRO}
%\cite{xu2012paraphrasing,xu2014data} also provide a small dictionary of approximate Shakespearean English - Modern English word pairs, crawled from \url{shakespeare-words.com}, a source distinct from Sparknotes. Since the two second persons and their corresponding forms (thy, thou, thyself etc) are very frequent but not present in this dictionary, we add these 4 pairs. Otherwise, we use the dictionary as it is. The final dictionary we use has 1524 pairs. \cite{faruqui2014retrofitting} had proposed a  method to update a learnt embedding to incorporate lexicon constraints. We use their off-the-shelf 
%implementation \footnote{\url{https://github.com/mfaruqui/retrofitting}} to incorporate the dictionary constraints into our pretrained embeddings.

%\subsubsection{RETROEXT}

%\subsubsection{METRO}
%\subsubsection{METROEXT}


%We use the python gensim \footnote{\url{https://radimrehurek.com/gensim/}} toolkit's implementation of word2vec to train our embeddings. We use four distinct strategies to train our data.

%In the case where we use external data, we first train the embeddings using both the external data and training data, and then for the same number of iterations on training data alone, to ensure adaptation.

%Note that we cannot directly use off-the-shelf pre-trained embeddings such as \textit{Glove} \cite{pennington2014glove} and \textit{Word2Vec} \cite{mikolov2013efficient} since we need to learn embeddings for novel word forms (and also different word senses for extant word forms) in Early Modern English. %Also, we are hampered by the lack of a standard monolingual dataset for Shakespearean English.

\subsection{Training and Parameters}
We use a minibatch-size of $32$ and the \textit{ADAM} optimizer \cite{kingma2014adam} with learning rate $0.001$,  momentum parameters $0.9$ and $0.999$, and $\epsilon=10^{-8}$. We did not use dropouts for any of our models. All our implementations are coded in Tensorflow 1.1.0 and run on a NVIDIA TitanX GPU. Both our encoder and decoder LSTMs are one layer deep. We do not use multi-layer/stacked LSTMs \cite{luong2015effective} given the paucity of parallel training data.


For every model, we experiment with two configurations of embedding and LSTM size - $S$ (128-128), $M$ (192-192) and $L$ (256-256). Across models, we find that the $M$ configuration performs better in terms of highest validation BLEU.  We also find that larger configurations (384-384 \& 512-512) fail to converge or perform very poorly \footnote{This is expected given the small parallel data}. Here, we report results only for the $M$ configuration for all the models.


\subsection{Validation and Tuning}
For all our models, we pick the best saved model over 15 epochs which has the highest validation BLEU. %For models with SL, $\lambda=0.0025$ is found to work well based on validation set.

\subsection{Decoding}
At test-time we use greedy decoding to find the most likely target sentence\footnote{Empirically, we observed that beam search does not give improvements for our task}. We also experiment with a post-processing strategy which replaces \emph{UNKs} in the target output with the highest aligned (maximum attention) source word. We find that this gives a small jump in \emph{BLEU} of about 0.1-0.2 for all neural models \footnote{Since effect is small and uniform, we report BLEU before post-processing in Table \ref{tab:knightExp} }. Our best model, for instance, gets a jump of 0.14 to reach a BLEU of \emph{\textbf{31.26}} from 31.12.



