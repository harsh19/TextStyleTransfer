
\section{Related Work} \label{sec:RelatedWord}

%style adaptation
There have been some prior work on style adaptation. Xu et al. \shortcite{xu2012paraphrasing} use phrase table based statistical machine translation to transform text to target style.  % \cite{xu2014data}
On the other hand our method is an end-to-end trainable neural network.
Saha Roy et al \shortcite{saha2015automated} leverage different language models based geolocation and occupation to align a text to specific style. However, their work is limited to addition of adjectives and adverbs. Our method can handle more generic tranformations including addition and deletion of words.

S2S neural models were first proposed by \cite{sutskever2014sequence}. Furthermore, \cite{bahdanau2014neural} enriched these methods with a attention mechanism , giving state-of-the-art results for MT. Since then, these models have been applied to many tasks like parsing \cite{vinyals2015pointer} and summarization \cite{rush2015neural}. In the context of MT, various settings such as multi-source MT \cite{zoph2016multi}, many-to-many MT \cite{johnson2016google}, MT with external information \cite{sennrich2016controlling} have been explored. Distinct from all of these, our work attempts to solve a Modern $\rightarrow$ Original English style transfer task. Although our task is closely related to both paraphrasing and MT, it has some unique task-specific properties such as considerable source-target overlap in vocabulary and grammar (unlike MT), while at the same time not sharing the source and target language (unlike paraphrasing). Unlike text simplification and summarization, our task does not involve shortening content length. 

Pointer networks \cite{vinyals2015pointer} allow the use of input-side words directly as output in a neural S2S model, and have been used for tasks like extractive summarization \cite{see2017get} \cite{zeng2016efficient}  and question answering \cite{wang2016machine}. However, pointer networks cannot generate words not present in the input. A mixture model of recurrent neural network and pointer network has been shown to achieve good performance on language modeling task \cite{merity2016pointer}. Gu et al \shortcite{gu2016incorporating} also proposed a mixture model pointer network - \textit{COPYNET} for sequence to sequence tasks. A major motivation for the development of pointer networks was their ability to easily copy tokens corresponding to rare words, proper nouns and entity names from the input-side. Distinct from the prior work on such networks, the high source-target language overlap for our task provides an added motivation to use them, and our experiments illustrate this utility.

